A little fun with Danish CPR (social security) numbers.

Apparently, some people believe that storing hashes of CPR numbers is a safe
thing to do, since cryptographic hashes are irreversible. A little knowledge is
a dangerous thing, as the saying goes.

Note: this document is pretty old, and could use some love. More recent stuff
has been added to benchmark.txt, dbgen.txt and todo.txt.

This little toy project aims to show why such a practice is a bad idea, and to
act as an excuse for doing a little C++ programming and familiarizing myself
with some C++11 features. Oh, and testing Microsofts new Git support in VS2012.

Upon initial observation, a CPR number consists of 10 numeric digits. A really
naÃ¯ve bruteforce would be a 10^10 exhaustive search, which is a fairly big
number - but even assuming a (low!) 50k attempts/sec, that'd still only be ~55
hours... clearly not secure enough. The purpose of this project is to show just
how fast we can do the brute-forcing with a series of simple optimizations, to
drive the point home.

We clearly don't have to brute the entire 10^10 space for CPR numbers, though.
The first six digits are DD/MM/YY, which reduces the key space a lot. The last
four digits used to be a MOD11 code, but these days I believe the entire 10k
range is valid. The YY part is "somewhat special", but basically the exhaustive
key space is reduced to 366*100*10000 - because I'm lazy, I won't do exact leap
year calculations during the bruteforce (at least not initially), since it can
be filtered afterwards anyway.

Having reduced the key space to 366 million entries, we're down to ~2 hours
exhaustive search for our hypothetical 50k attemps/sec guesstimate. But lets
get cracking and see what kind of real-world speed we can reach :-)

Project breakdown:
==================
1) project setup, getting the most fundamental core routines done.
2) tool to print hash of a given CPR number.
3) single-threaded (relatively) naÃ¯ve bruteforcer.
4) optimization.
5) initial multi-threaded implementation.
6) optimization.
7) pre-gen hash->cpr table to sqlite database, lookup tool.
8) linux port?

Considerations:
===============
*) Shortcuts *will* be taken, for simplicity and for speed.
*) Portability will be kept in mind (attempting no Windowsisms) but I won't be
	going all-in (unaligned memory access is likely to happen).
*) Ability to support alternate hashing functions will be given a bit of
	thought, but won't be given priority during initial design.
*) Try to keep permutation iteration separate from action right from the start.
	Probably using std::function - now we finally have them, we do want to use
	lambdas, right? :-)

Future ideas:
=============
For the precomputed "dbtool", a content-addressable version might be interesting.
Simply create a file for each hash, with the CPR string as content, possibly with
a bit of directory hierarchy to not put millions of files in one subdirectory.
Per-file content would be small, so should be interned in the MFT, and we get
B-tree indexing automatically. OTOH each NTFS file record is 1k, so the disk
overhead will be wasteful compared to a SQLite database. But it should be viable,
quoting John Carmack https://twitter.com/ID_AA_Carmack/status/138676625729527808
"Somewhat surprisingly, win7 does just fine with 100,000 files in a directory
(content addressable storage system)".

Benchmarking:
=============
Tests run on an i7-3770 at stock 3.40GHz speed, win7-x64, bruteforce compiled
for x86 target. Single-threaded versions have been affinity-limited to one core
and HIGH thread priority.

v0.3	=> ~405-425k hashops/sec
v0.4.0	=> ~992k hashops/sec		(stringstream -> chararray + sprintf)			~2.390x
v0.4.1	=> ~1025k hashops/sec		(avoid string-from-char* in inner loop)			~1.033x
v0.4.2	=> ~1108k hashops/sec		(core/hashFromCpr: isValidCpr check->assert)	~1.081x
v0.4.3	=> ~1260k hashops/sec		(ddmm string computation -> lookup table)		~1.138x
v0.4.4	=> ~1611k hashops/sec		(sprintf -> base10fixWidthStr)					~1.279x
v0.4.5	=> ~1644k hashops/sec		(runpermutations: string -> char array)			~1.020x
